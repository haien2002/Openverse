# main.py

# arxivæœ€æ–°æ–‡çŒ®çš„æŠ“å–ä¸LLMæ‹†è§£

# æ ¸å¿ƒå‚æ•°
# DAYS_TO_FETCH = å‡ å¤©å†…å‘è¡¨çš„æ–‡çŒ®
# PAPERS_PER_CATEGORY = è¿”å›çš„æ–‡çŒ®æ•°é™åˆ¶ï¼ŒNoneå¯ä»¥å…¨å‡º

# syl 20250627

import arxiv
import websocket
import datetime
import hashlib
import hmac
import base64
import json
import ssl
import sqlite3
import os
import time
import random
from urllib.parse import urlparse, urlencode
import threading

# ==============================================================================
# 1. é…ç½®éƒ¨åˆ†
# ==============================================================================
APPID = "548a4e52"
API_KEY = "abc0273b4e1c403f4849dc318b281464"
API_SECRET = "NGViNTE3N2FhYTVkOTQyZTQ0MzFhOGI4"
SPARK_URL = "wss://spark-api.xf-yun.com/v3.1/chat" 
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_FILE = os.path.join(BASE_DIR, "arxiv_papers.db")

# ==============================================================================
# 2. æ•°æ®åº“ç®¡ç†æ¨¡å— (è±ªåç‰ˆ)
# ==============================================================================
def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼Œåˆ›å»ºæˆ–æ›´æ–°è®ºæ–‡è¡¨ç»“æ„ã€‚"""
    print(f"ğŸ—„ï¸ æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“ '{DB_FILE}'...")
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS papers (
                id TEXT PRIMARY KEY, category TEXT NOT NULL, title TEXT NOT NULL,
                url TEXT NOT NULL UNIQUE, published TEXT NOT NULL, original_summary TEXT,
                spark_summary TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                authors TEXT, comment TEXT, journal_ref TEXT, doi TEXT
            )
        ''')
        existing_columns = [col[1] for col in cursor.execute("PRAGMA table_info(papers)").fetchall()]
        new_columns = { "authors": "TEXT", "comment": "TEXT", "journal_ref": "TEXT", "doi": "TEXT" }
        for col, col_type in new_columns.items():
            if col not in existing_columns:
                cursor.execute(f"ALTER TABLE papers ADD COLUMN {col} {col_type}")
        conn.commit()
    print("   - æ•°æ®åº“è¡¨ç»“æ„å‡†å¤‡å°±ç»ªã€‚")

def save_papers_to_db(papers_data):
    """å°†è®ºæ–‡æ•°æ®åˆ—è¡¨ä¿å­˜åˆ° SQLite æ•°æ®åº“ä¸­ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™æ›´æ–°ã€‚"""
    if not papers_data: return
    print(f"ğŸ’¾ å‡†å¤‡å°† {len(papers_data)} ç¯‡è®ºæ–‡æ•°æ®å­˜å…¥æ•°æ®åº“...")
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        for paper in papers_data:
            paper_id = paper['url'].split('/abs/')[-1]
            cursor.execute('''
                INSERT OR REPLACE INTO papers (id, category, title, url, published, original_summary, 
                    spark_summary, authors, comment, journal_ref, doi)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                paper_id, paper.get('category', 'N/A'), paper.get('title', 'N/A'),
                paper.get('url', 'N/A'), paper.get('published', 'N/A'),
                paper.get('original_summary', ''), paper.get('spark_summary', ''),
                paper.get('authors', ''), paper.get('comment', ''),
                paper.get('journal_ref', ''), paper.get('doi', '')
            ))
        conn.commit()
    print(f"   - æ•°æ®ä¿å­˜/æ›´æ–°æˆåŠŸï¼")

# ==============================================================================
# 3. ArXiv è®ºæ–‡æŠ“å–æ¨¡å— (åä¾¦å¯Ÿç‰ˆ)
# ==============================================================================
def fetch_papers_in_range_robust(query, start_date, end_date):
    """åœ¨ä¸€ä¸ªå°æ—¶é—´æ®µå†…ï¼Œä»¥éå¸¸ç¨³å¥çš„æ–¹å¼æŠ“å–è®ºæ–‡ï¼ŒåŒ…å«æ™ºèƒ½é‡è¯•å’ŒæŒ‡æ•°é€€é¿æœºåˆ¶ã€‚"""
    precise_query = f"({query}) AND submittedDate:[{start_date.strftime('%Y%m%d%H%M%S')} TO {end_date.strftime('%Y%m%d%H%M%S')}]"
    print(f"   - æ­£åœ¨æŸ¥è¯¢æ—¶é—´æ®µ: {start_date.date()} åˆ° {end_date.date()}")
    
    max_retries = 5
    base_delay = 10
    
    for attempt in range(max_retries):
        try:
            client = arxiv.Client(page_size=200, delay_seconds=5.0, num_retries=3)
            search = arxiv.Search(query=precise_query, sort_by=arxiv.SortCriterion.SubmittedDate)
            
            papers_in_chunk = []
            for result in client.results(search):
                author_names = ", ".join([author.name for author in result.authors])
                papers_in_chunk.append({
                    "title": result.title, "summary": result.summary.replace('\n', ' '),
                    "published": result.published.strftime("%Y-%m-%d"), "url": result.entry_id,
                    "authors": author_names, "comment": result.comment,
                    "journal_ref": result.journal_ref, "doi": result.doi
                })
            
            print(f"     âœ… åœ¨æ­¤æ—¶é—´æ®µå†…æ‰¾åˆ° {len(papers_in_chunk)} ç¯‡è®ºæ–‡ã€‚")
            return papers_in_chunk
        except Exception as e:
            print(f"     âŒ ç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•å¤±è´¥: ä¾¦æµ‹åˆ°æœåŠ¡å™¨è¿æ¥é—®é¢˜ã€‚")
            if attempt < max_retries - 1:
                sleep_time = base_delay * (2 ** attempt) + random.uniform(0, 5)
                print(f"     ...å¯åŠ¨åä¾¦å¯Ÿç­–ç•¥ï¼Œå°†æš‚åœ {sleep_time:.2f} ç§’åé‡è¯•...")
                time.sleep(sleep_time)
            else:
                print(f"     âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè·³è¿‡æ­¤æ—¶é—´æ®µã€‚é”™è¯¯: {e}")
                return []
    return []

# ==============================================================================
# 4. è®¯é£æ˜Ÿç« WebSocket å®¢æˆ·ç«¯ (ä¿æŒä¸å˜)
# ==============================================================================
class SparkClient:
    def __init__(self):
        self.appid = APPID; self.api_key = API_KEY; self.api_secret = API_SECRET
        self.spark_url = SPARK_URL; self.domain = "generalv3"; self.result_text = ""
        self.is_ws_closed = threading.Event()
    def _generate_auth_url(self):
        host, path = urlparse(self.spark_url).netloc, urlparse(self.spark_url).path
        now = datetime.datetime.now(datetime.timezone.utc).strftime('%a, %d %b %Y %H:%M:%S GMT')
        signature_origin = f"host: {host}\ndate: {now}\nGET {path} HTTP/1.1"
        signature_sha = hmac.new(self.api_secret.encode('utf-8'), signature_origin.encode('utf-8'), digestmod=hashlib.sha256).digest()
        signature_sha_base64 = base64.b64encode(signature_sha).decode('utf-8')
        authorization_origin = f'api_key="{self.api_key}", algorithm="hmac-sha256", headers="host date request-line", signature="{signature_sha_base64}"'
        authorization = base64.b64encode(authorization_origin.encode('utf-8')).decode('utf-8')
        return self.spark_url + "?" + urlencode({ "authorization": authorization, "date": now, "host": host })
    def _on_message(self, ws, message):
        data = json.loads(message); code = data.get('header', {}).get('code', -1)
        if code != 0: print(f"Spark Error: {code}"); ws.close()
        content = data.get('payload', {}).get('choices', {}).get('text', [{}])[0].get('content', '')
        self.result_text += content
        if data.get('header', {}).get('status') == 2: ws.close()
    def _on_error(self, ws, error): print(f"Spark WebSocket Error: {error}"); self.is_ws_closed.set()
    def _on_close(self, ws, close_status_code, close_msg): self.is_ws_closed.set()
    def _on_open(self, ws, prompt):
        threading.Thread(target=lambda: ws.send(json.dumps({
            "header": {"app_id": self.appid, "uid": "harvester_user"},
            "parameter": {"chat": {"domain": self.domain, "temperature": 0.5, "max_tokens": 2048}},
            "payload": {"message": {"text": [{"role": "user", "content": prompt}]}}
        }))).start()
    def summarize_text(self, text_to_summarize):
        self.result_text = ""; self.is_ws_closed.clear()
        prompt = f"ä½ æ˜¯ä¸€ä½é¡¶å°–çš„ç§‘ç ”åŠ©ç†ï¼Œè¯·ä½¿ç”¨ä¸­æ–‡ï¼Œä»¥ä¸“ä¸šã€ç²¾ç‚¼ã€é«˜åº¦æ¦‚æ‹¬çš„è¯­è¨€é£æ ¼ï¼Œä¸ºä»¥ä¸‹ç§‘ç ”è®ºæ–‡æ‘˜è¦æç‚¼å‡º3ä¸ªæœ€é‡è¦çš„æ ¸å¿ƒè¦ç‚¹ï¼Œå¹¶ä»¥ Markdown çš„æ— åºåˆ—è¡¨æ ¼å¼å‘ˆç°ã€‚\n\næ‘˜è¦å†…å®¹ï¼š\n\"\"\"\n{text_to_summarize}\n\"\"\""
        ws = websocket.WebSocketApp(self._generate_auth_url(), on_message=self._on_message, on_error=self._on_error, on_close=self._on_close, on_open=lambda ws: self._on_open(ws, prompt))
        ws.run_forever(sslopt={"cert_reqs": ssl.CERT_NONE}); self.is_ws_closed.wait()
        return self.result_text.strip()

# ==============================================================================
# 5. ä¸»ç¨‹åºå…¥å£
# ==============================================================================
def main_harvester():
    """è¿è¡Œå·¥ä¸šçº§çš„æ•°æ®é‡‡é›†å’Œå¤„ç†æµç¨‹ã€‚"""
    if APPID == "ä½ çš„_APPID": print("ğŸ›‘ è¯·å…ˆåœ¨è„šæœ¬é¡¶éƒ¨å¡«å†™ä½ çš„è®¯é£æ˜Ÿç« API å¯†é’¥ä¿¡æ¯ï¼"); return
    
    init_database()
    
    # --- âœ¨ åœ¨è¿™é‡Œé…ç½®ä½ çš„â€œæŒ‡å®šèˆªåŒºâ€å‚æ•° ---
    # ä¾‹å¦‚ï¼šæŠ“å– 180 å¤©å‰ åˆ° 540 å¤©å‰çš„æ•°æ®
    FETCH_END_DAYS_AGO = 180      # èˆªè¡Œç»“æŸç‚¹ (è·ç¦»ä»Šå¤©çš„å¤©æ•°)
    FETCH_START_DAYS_AGO = 540    # èˆªè¡Œèµ·å§‹ç‚¹ (è·ç¦»ä»Šå¤©çš„å¤©æ•°)
    
    CHUNK_SIZE_DAYS = 30 # æ¯æ¬¡APIè¯·æ±‚çš„æ—¶é—´çª—å£å¤§å°ï¼ˆå¤©ï¼‰
    
    # --- è®¡ç®—èˆªè¡Œå‚æ•° ---
    now = datetime.datetime.now(datetime.timezone.utc)
    window_end_date = now - datetime.timedelta(days=FETCH_END_DAYS_AGO)
    total_duration_days = FETCH_START_DAYS_AGO - FETCH_END_DAYS_AGO
    
    if total_duration_days <= 0:
        print("âŒ é”™è¯¯ï¼šæŠ“å–èµ·å§‹å¤©æ•°å¿…é¡»å¤§äºç»“æŸå¤©æ•°ã€‚")
        return

    print(f"ğŸš¢ â€œå¤§èˆªæµ·â€ä»»åŠ¡å¯åŠ¨ï¼Œç›®æ ‡èˆªåŒºï¼š{FETCH_START_DAYS_AGO}å¤©å‰ è‡³ {FETCH_END_DAYS_AGO}å¤©å‰ã€‚æ€»èˆªç¨‹ï¼š{total_duration_days}å¤©ã€‚")

    TARGET_CATEGORIES = {"Biomolecules (ç”Ÿç‰©åˆ†å­)": "cat:q-bio.BM", "Cell Behavior (ç»†èƒè¡Œä¸º)": "cat:q-bio.CB", "Genomics (åŸºå› ç»„å­¦)": "cat:q-bio.GN", "Molecular Networks (åˆ†å­ç½‘ç»œ)": "cat:q-bio.MN", "Neurons and Cognition (ç¥ç»å…ƒä¸è®¤çŸ¥)": "cat:q-bio.NC", "Other Quantitative Biology (å…¶ä»–å®šé‡ç”Ÿç‰©å­¦)": "cat:q-bio.OT", "Populations and Evolution (ç§ç¾¤ä¸è¿›åŒ–)": "cat:q-bio.PE", "Quantitative Methods (å®šé‡æ–¹æ³•)": "cat:q-bio.QM", "Subcellular Processes (äºšç»†èƒè¿‡ç¨‹)": "cat:q-bio.SC", "Tissues and Organs (ç»„ç»‡ä¸å™¨å®˜)": "cat:q-bio.TO"}
    spark_summarizer = SparkClient()
    
    for category_name, category_query in TARGET_CATEGORIES.items():
        print(f"\n\n{'='*80}\nğŸ§¬ å¼€å§‹å¤„ç†åˆ†ç±»: {category_name}\n{'='*80}")
        
        all_papers_in_category = []
        
        # æŒ‰æ—¶é—´åˆ†ç‰‡è¿›è¡Œè¿­ä»£æŠ“å–
        for i in range(0, total_duration_days, CHUNK_SIZE_DAYS):
            chunk_end_date = window_end_date - datetime.timedelta(days=i)
            chunk_start_date = window_end_date - datetime.timedelta(days=i + CHUNK_SIZE_DAYS)
            
            papers_chunk = fetch_papers_in_range_robust(category_query, chunk_start_date, chunk_end_date)
            all_papers_in_category.extend(papers_chunk)
            print(f"   --- å®Œæˆä¸€ä¸ªæ—¶é—´å—çš„æŸ¥è¯¢ï¼Œç¤¼è²Œæ€§æš‚åœ3ç§’ ---")
            time.sleep(3)
        
        if not all_papers_in_category:
            print(f"  - åœ¨åˆ†ç±» '{category_name}' çš„æŒ‡å®šæ€»æ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡ã€‚")
            continue
            
        print(f"\nâœ¨ åœ¨åˆ†ç±» '{category_name}' ä¸‹å…±æŠ“å–åˆ° {len(all_papers_in_category)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹è¿›è¡ŒAIæ‘˜è¦...")
        
        processed_papers_for_db = []
        for i, paper in enumerate(all_papers_in_category):
            print(f"   --- æ­£åœ¨æ‘˜è¦ç¬¬ {i+1}/{len(all_papers_in_category)} ç¯‡: {paper['title'][:50]}...")
            spark_summary = spark_summarizer.summarize_text(paper['summary'])
            paper.update({"category": category_name, "spark_summary": spark_summary})
            processed_papers_for_db.append(paper)

        save_papers_to_db(processed_papers_for_db)

    print(f"\n\n{'='*80}\nğŸ‰ æ­å–œï¼â€œæŒ‡å®šèˆªåŒºâ€ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼æ‰€æœ‰æ•°æ®å‡å·²å­˜å…¥æ•°æ®åº“ '{DB_FILE}'\n{'='*80}")

if __name__ == '__main__':
    main_harvester()
